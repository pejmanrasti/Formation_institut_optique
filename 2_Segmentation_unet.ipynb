{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pejmanrasti/Formation_institut_optique/blob/main/2_Segmentation_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxJNtcLHyxK7"
      },
      "source": [
        "## Training a Unet\n",
        "\n",
        "In this notebook, we will train a 2D U-net for nuclei segmentation in the Kaggle Nuclei dataset.\n",
        "\n",
        "It is still possible to do this exercise on the CPU, but you will need some patience to wait for the training. That's why we have added GPU support.\n",
        "Please switch your Notebook to GPU in Runtime -> change runtime type -> T4 GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5CNxOpgx-ey"
      },
      "source": [
        "## The libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK0BTeevRv_N"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from itertools import chain\n",
        "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
        "from skimage.transform import resize\n",
        "from skimage.morphology import label\n",
        "\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFTBWHvqLYwL"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "\n",
        "For this exercise we will be using the Kaggle 2018 Data Science Bowl data again, but this time we will try to segment it with the state of the art network.\n",
        "Let's start with loading the data as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k82ftNngK8Qf"
      },
      "source": [
        "# Data download and unzipping\n",
        "!wget https://raw.githubusercontent.com/AakashSudhakar/2018-data-science-bowl/master/compressed_files/stage1_test.zip -c\n",
        "!wget https://raw.githubusercontent.com/AakashSudhakar/2018-data-science-bowl/master/compressed_files/stage1_train.zip -c\n",
        "\n",
        "!mkdir stage1_train stage1_test\n",
        "\n",
        "# Suppress output with the -q flag\n",
        "!unzip -q stage1_train.zip -d stage1_train/\n",
        "!unzip -q stage1_test.zip -d stage1_test/\n",
        "\n",
        "# Data Path\n",
        "TRAIN_PATH = 'stage1_train/'\n",
        "TEST_PATH = 'stage1_test/'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuRXDRJsMxVo"
      },
      "source": [
        "__TASK__: Use `ls` to explore the contents of both folders. Running `ls your_folder_name` should display you what is stored in the folder of your interest.\n",
        "\n",
        " How are the images stored? What format do they have? What about the ground truth (the annotation masks)? Which format are they stored in?\n",
        "\n",
        "Hint: you can use the following function to display the images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehovgGj1NQrG"
      },
      "source": [
        "def show_one_image(image_path):\n",
        "  image = imageio.imread(image_path)\n",
        "  plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7WFCKqDUGgC"
      },
      "source": [
        "What one would normally start with in any machine learning pipeline is writing a dataset - a class that will fetch the training samples. In the previous exercises we did not have to worry about it, since we used the classic datasets available in the torchvision library. However, once you switch to using your own data, you would have to figure out how to fetch the data yourself. Luckily most of the functionality is already provided, but what you need to do is to write a class, that will actually supply the dataloader with training samples - a Dataset.\n",
        "\n",
        "For this exercise you will not have to do it yourself yet, but please carefully read through the provided class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsSW56Rh7ZwA"
      },
      "source": [
        "# Set some parameters\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "IMG_CHANNELS = 3\n",
        "TRAIN_PATH = 'stage1_train/'\n",
        "TEST_PATH = 'stage1_test/'\n",
        "\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sGsnkG0GB-n"
      },
      "source": [
        "def get_data(path, train=True):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses image data.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the directory containing the image data.\n",
        "        train (bool, optional): Flag indicating if the data is for training.\n",
        "                                 Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        tuple or ndarray: If train is True, returns a tuple containing the\n",
        "                          image data (X) and corresponding masks (Y).\n",
        "                          If train is False, returns only the image data (X).\n",
        "    \"\"\"\n",
        "    # Get the list of image IDs\n",
        "    ids = next(os.walk(path))[1]\n",
        "\n",
        "    # Initialize arrays to store image data and masks\n",
        "    X = np.zeros((len(ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "\n",
        "    # Initialize masks array only if train is True\n",
        "    if train:\n",
        "        Y = np.zeros((len(ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=bool)\n",
        "\n",
        "    print('Getting and resizing images ... ')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    # Iterate through each image ID\n",
        "    for n, id_ in tqdm(enumerate(ids), total=len(ids)):\n",
        "        path_new = path + id_\n",
        "\n",
        "        # Read and resize the image\n",
        "        img = imread(path_new + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n",
        "        img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
        "        X[n] = img\n",
        "\n",
        "        # Read and process masks only if train is True\n",
        "        if train:\n",
        "            mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=bool)\n",
        "            for mask_file in next(os.walk(path_new + '/masks/'))[2]:\n",
        "                mask_ = imread(path_new + '/masks/' + mask_file)\n",
        "                mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant',\n",
        "                                              preserve_range=True), axis=-1)\n",
        "                mask = np.maximum(mask, mask_)\n",
        "            Y[n] = mask\n",
        "\n",
        "    # Return image data and masks (if train is True) or only image data\n",
        "    if train:\n",
        "        return X, Y\n",
        "    else:\n",
        "        return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bAA6AwnUjqp"
      },
      "source": [
        "Now let's load the dataset and visualize it by calling our function:\n",
        "\n",
        "In this example, we read all images of the train folder as training data (applied SGD on) and all images of the validation folder for testing data (report performance on). Validation data (optimize hyper-parameters on) will be taken randomly from training data during the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jda9EBbmHUNy"
      },
      "source": [
        "X, Y = get_data(TRAIN_PATH, train=True)\n",
        "X_test = get_data(TEST_PATH, train=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and validation sets\n",
        "split_ratio = 0.8  # Example: 80% for training, 20% for validation\n",
        "split_index = int(len(X) * split_ratio)\n",
        "\n",
        "X_train = X[:split_index]\n",
        "Y_train = Y[:split_index]\n",
        "X_val = X[split_index:]\n",
        "Y_val = Y[split_index:]"
      ],
      "metadata": {
        "id": "U5X-EFnoelRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training data shape: X={X_train.shape}, Y={Y_train.shape}\")\n",
        "print(f\"Validation data shape: X={X_val.shape}, Y={Y_val.shape}\")\n",
        "print(f\"Test data shape: X={X_test.shape}\")"
      ],
      "metadata": {
        "id": "nQf1NcMSdry5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyve1KK4-sS8"
      },
      "source": [
        "# Check if training data looks all right\n",
        "ix = random.randint(0, len(X_train))\n",
        "imshow(X_train[ix])\n",
        "plt.show()\n",
        "imshow(np.squeeze(Y_train[ix]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28ZjLuyGXXPL"
      },
      "source": [
        "## Building a U-NET model\n",
        "Now we need to define the architecture of the model to use. This time we will use a [U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) that has proven to steadily outperform the other architectures in segmenting biological and medical images.\n",
        "\n",
        "The U-net has an encoder-decoder structure:\n",
        "\n",
        "In the encoder pass, the input image is successively downsampled via max-pooling. In the decoder pass it is upsampled again via transposed convolutions.\n",
        "\n",
        "In adddition, it has skip connections, that bridge the output from an encoder to the corresponding decoder.\n",
        "\n",
        "Note that we are using valid convolutions here; the input to convolutions are not padded and the spatial output size after applying them decreases. Hence, the spatial output size of the network will be smaller than the spatial input size. This could be avoided by using same convolutions, which would increase the computational effort though.\n",
        "\n",
        "Compared to the paper, we will use less features (channels) to enable training the network on the CPU as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG7qWUaZDGX7"
      },
      "source": [
        "#Each block of u-net architecture consist of two Convolution layers\n",
        "# These two layers are written in a function to make our code clean\n",
        "def conv2d_block(input_tensor, n_filters, kernel_size=3):\n",
        "    # first layer\n",
        "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size),\n",
        "               padding=\"same\")(input_tensor)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    # second layer\n",
        "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size),\n",
        "               padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmmcdboyDOfy"
      },
      "source": [
        "# The u-net architecture consists of contracting and expansive paths which\n",
        "# shrink and expands the inout image respectivly.\n",
        "# Output image have the same size of input image\n",
        "def get_unet(input_img, n_filters):\n",
        "    # contracting path\n",
        "    c1 = conv2d_block(input_img, n_filters=n_filters*4, kernel_size=3) #The first block of U-net\n",
        "    p1 = MaxPooling2D((2, 2)) (c1)\n",
        "\n",
        "    c2 = conv2d_block(p1, n_filters=n_filters*8, kernel_size=3)\n",
        "    p2 = MaxPooling2D((2, 2)) (c2)\n",
        "\n",
        "    c3 = conv2d_block(p2, n_filters=n_filters*16, kernel_size=3)\n",
        "    p3 = MaxPooling2D((2, 2)) (c3)\n",
        "\n",
        "    c4 = conv2d_block(p3, n_filters=n_filters*32, kernel_size=3)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "\n",
        "    c5 = conv2d_block(p4, n_filters=n_filters*64, kernel_size=3) # last layer on encoding path\n",
        "\n",
        "    # expansive path\n",
        "    u6 = Conv2DTranspose(n_filters*32, (3, 3), strides=(2, 2), padding='same') (c5) #upsampling included\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = conv2d_block(u6, n_filters=n_filters*32, kernel_size=3)\n",
        "\n",
        "    u7 = Conv2DTranspose(n_filters*16, (3, 3), strides=(2, 2), padding='same') (c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = conv2d_block(u7, n_filters=n_filters*16, kernel_size=3)\n",
        "\n",
        "    u8 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = conv2d_block(u8, n_filters=n_filters*8, kernel_size=3)\n",
        "\n",
        "    u9 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c8)\n",
        "    u9 = concatenate([u9, c1], axis=3)\n",
        "    c9 = conv2d_block(u9, n_filters=n_filters*4, kernel_size=3)\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
        "    model = Model(inputs=[input_img], outputs=[outputs])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjAen-hlbLdb"
      },
      "source": [
        "## Loss and distance metrics\n",
        "\n",
        "The next step to do would be writing a loss function - a metric that will tell us how close we are to the desired output. This metric should be differentiable, since this is the value to be backpropagated. The are [multiple losses](https://lars76.github.io/neural-networks/object-detection/losses-for-segmentation/) we could use for the segmentation task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLBwBMipbLdl"
      },
      "source": [
        "We use Binary Cross Entropy averaged over pixels as training loss.\n",
        "This loss function is similar to the cross entropy loss we have used\n",
        "for the previous classification tasks.\n",
        "\n",
        "The difference to these tasks is that we predict a single number per pixel\n",
        "(the probability of this pixel being foreground / background) instead of\n",
        "a vector per image that encodes the probabilities for several classes.\n",
        "\n",
        "We also will use the [Dice Coefficeint](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) to evaluate the network predictions.\n",
        "We can use it for validation if we interpret set $a$ as predictions and $b$ as labels. It is often used to evaluate segmentations with sparse foreground, because the denominator normalizes by the number of foreground pixels.\n",
        "The Dice Coefficient is closely related to Jaccard Index / Intersection over Union."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2ypLh2eDaFg"
      },
      "source": [
        "# the coefficient takes values in [0, 1], where 0 is the worst score, 1 is the best score\n",
        "# the dice coefficient of two sets represented as vectors a, b ca be computed as (2 *|a b| / (a^2 + b^2))\n",
        "def dice_coefficient(y_true, y_pred):\n",
        "    eps = 1e-6\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    #Casting to float32 to resolve the type error\n",
        "    y_true_f = tf.cast(y_true_f, dtype=tf.float32)\n",
        "    y_pred_f = tf.cast(y_pred_f, dtype=tf.float32)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    # Ensure denominator is not zero by adding a small value(1e-6) to avoid division by zero errors\n",
        "    result = (2. * intersection + 1e-6) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f) + 1e-6)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C5sXunADfs-"
      },
      "source": [
        "# Creating and Compiling the model\n",
        "input_img = Input((X_train.shape[1], X_train.shape[2], 3), name='img')\n",
        "model = get_unet(input_img, n_filters=4)\n",
        "\n",
        "model.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=[dice_coefficient])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AESOWsYnEZCt"
      },
      "source": [
        "# Fiting the model\n",
        "results = model.fit(X_train, Y_train,\n",
        "                    batch_size=5, epochs=15,\n",
        "                    validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKo4jqpYeHLK"
      },
      "source": [
        "## Model testing and predictions\n",
        "Now this is the time to evaluate our training model on test data which the model has never seen them before. In Keras, we can use \"model.evaluate\" to evaluate the training model where there is an avalibility of masks of test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jse1rUxRSQr3"
      },
      "source": [
        "model.evaluate(X_val,Y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L17xt0U-ehA9"
      },
      "source": [
        "In Keras, \"model.predict\" is the function to predict output (masks in segmentation task or labels in classification task). Then we visualize results and visually compare the predicted masks with the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMiuCBWAFJFI"
      },
      "source": [
        "preds_val = model.predict(X_val, verbose=1)\n",
        "# we apply a threshold on predicted mask (probability mask) to convert it to a binary mask.\n",
        "preds_val_t = (preds_val > 0.3).astype(np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILtSa0-OQYMo"
      },
      "source": [
        "ix = random.randint(0, len(X_val))\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "plt.subplot(221)\n",
        "plt.imshow(X_val[ix,:,:,0])\n",
        "plt.title(\"input image\")\n",
        "plt.subplot(222)\n",
        "plt.imshow(np.squeeze(Y_val[ix, :, :, 0]))\n",
        "plt.title(\"ground truth\")\n",
        "plt.subplot(223)\n",
        "plt.imshow(np.squeeze(preds_val[ix, :, :, 0]))\n",
        "plt.title(\"Probability map of the predicted mask\")\n",
        "plt.subplot(224)\n",
        "plt.imshow(np.squeeze(preds_val_t[ix, :, :, 0]))\n",
        "plt.title(\"Predicted mask after thresholding\")\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_test = model.predict(X_test, verbose=1)\n",
        "# we apply a threshold on predicted mask (probability mask) to convert it to a binary mask.\n",
        "preds_test_t = (preds_test > 0.3).astype(np.uint8)\n",
        "\n",
        "ix = random.randint(0, len(X_test))\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "plt.subplot(131)\n",
        "plt.imshow(X_test[ix,:,:,0])\n",
        "plt.title(\"input image\")\n",
        "plt.subplot(132)\n",
        "plt.imshow(np.squeeze(preds_test[ix, :, :, 0]))\n",
        "plt.title(\"Probability map of the predicted mask\")\n",
        "plt.subplot(133)\n",
        "plt.imshow(np.squeeze(preds_test_t[ix, :, :, 0]))\n",
        "plt.title(\"Predicted mask after thresholding\")\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W19xGis9ZKdE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}