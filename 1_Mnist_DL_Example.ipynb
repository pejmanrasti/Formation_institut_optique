{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pejmanrasti/Formation_institut_optique/blob/main/1_Mnist_DL_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNk9xoejq0Gu"
      },
      "source": [
        "# **Make your fist experience with Tensorflow-Keras**\n",
        "Our goal is to construct and train an artificial neural network on thousands of images of handwritten digits so that it may successfully identify others when presented. The data that will be incorporated is the MNIST database which contains 60,000 images for training and 10,000 test images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19s9e7VgHhmR"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "! pip install livelossplot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEFL5miLsJAO"
      },
      "source": [
        "# **Importing necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wbe9-7F0llo4"
      },
      "outputs": [],
      "source": [
        "from livelossplot import PlotLossesKeras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential # Model type to be used\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout# Make Fully connected (FC) layers\n",
        "from tensorflow.keras.utils import to_categorical # NumPy related tools\n",
        "from tensorflow.keras.callbacks import TensorBoard  #Visulization of Accuracy and loss\n",
        "\n",
        "import numpy as np                   # advanced math library\n",
        "import matplotlib.pyplot as plt      # MATLAB like plotting routines\n",
        "import random                        # for generating random numbers\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUaYe-8hrq0a"
      },
      "source": [
        "## Loading Training and Validation Data\n",
        "\n",
        "The MNIST dataset is conveniently bundled within Keras, and we can easily analyze some of its features in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyugMSBellom"
      },
      "outputs": [],
      "source": [
        "# load (downloaded if needed) the MNIST dataset\n",
        "(X_train, y_train), (X_val_test, y_val_test) = mnist.load_data()\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42) # 0.5 means half of the validation data will be used for testing. Adjust as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j94bah8itXXQ"
      },
      "source": [
        "Visualization of some input images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2msmk9kllo8"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = (9,9) # Make the figures a bit bigger\n",
        "\n",
        "for i in range(9):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    num = random.randint(0, len(X_test))\n",
        "    plt.imshow(X_test[num], cmap='gray', interpolation='none')\n",
        "    plt.title(\"Class {}\".format(y_test[num]))\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_zCqwWUtsQk"
      },
      "source": [
        "## Formatting the input data layer\n",
        "\n",
        "Instead of a 28 x 28 matrix, we build our network to accept a 784-length vector. Each image needs to be then reshaped (or flattened) into a vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZcXG7IlllpF"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 784).astype('float32')\n",
        "X_val = X_val.reshape(X_val.shape[0], 784).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], 784).astype('float32')\n",
        "\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train /= 255.\n",
        "X_val /= 255.\n",
        "X_test /= 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMqOlzCvum99"
      },
      "source": [
        "We then modify our classes (unique digits) to be in the one-hot format, i.e."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noBosoMQupvp"
      },
      "outputs": [],
      "source": [
        "# one hot encode outputs\n",
        "Y_train = to_categorical(y_train)\n",
        "Y_val = to_categorical(y_val)\n",
        "Y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppNdrarWvSEV"
      },
      "source": [
        "# Building the simplest fully connected network (FCN) with just one layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytj_eL4zvFDB"
      },
      "outputs": [],
      "source": [
        "# The Sequential model is a linear stack of layers and is very common.\n",
        "model = Sequential([\n",
        "    Dense(10,input_shape=(784,)), # It is the output layer and should be equal to the number of desired classes (10 in this case).\n",
        "    Activation('softmax'),\n",
        "])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyJ_BhHrv52F"
      },
      "source": [
        "## Compiling the model\n",
        "\n",
        "Keras is built on top of TensorFlow. It allows you to define a *computation graph* in Python, which then compiles and runs efficiently on the CPU or GPU without the overhead of the Python interpreter.\n",
        "\n",
        "When compiling a model, Keras asks you to specify your **loss function** and your **optimizer**. The loss function we'll use here is called *categorical cross-entropy*, and is a loss function well-suited to comparing two probability distributions.\n",
        "\n",
        "Our predictions are probability distributions across the ten different digits (e.g. \"we're 80% confident this image is a 3, 10% sure it's an 8, 5% it's a 2, etc.\"), and the target is a probability distribution with 100% for the correct category, and 0 for everything else. The cross-entropy is a measure of how different your predicted distribution is from the target distribution. [More detail at Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_gOe1jFv4dk"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsVkxH97llpL"
      },
      "outputs": [],
      "source": [
        "plotlosses = PlotLossesKeras()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9UNb8kfyp9O"
      },
      "source": [
        "## Train the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBlmYIToyfwf"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, Y_train,\n",
        "          validation_data=(X_val, Y_val),\n",
        "          epochs=20, batch_size=32,\n",
        "          verbose=1,\n",
        "          callbacks=[plotlosses])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX5YLXkA1e66"
      },
      "source": [
        "## Evaluate Model's Accuracy on Test Data\n",
        "Your test data **Must** be different from the validation data, but in this example, we will use the validation data as the test data as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIaorIub1jw3"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(X_test, Y_test)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8zG9fGI35DI"
      },
      "source": [
        "### Inspecting the output\n",
        "\n",
        "It's always a good idea to inspect the output and make sure everything looks sane. Here we'll look at some examples it gets right, and some examples it gets wrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sZXC9YA1VUN"
      },
      "outputs": [],
      "source": [
        "print (model.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojQlAkh_36R0"
      },
      "outputs": [],
      "source": [
        "# The predict function outputs the probability for each class\n",
        "# according to the trained classifier for each input example.\n",
        "# np.argmax is used to get the class with the highest probability.\n",
        "predicted_classes = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "# Check which items we got right / wrong\n",
        "correct_indices = np.nonzero(predicted_classes == y_test)[0]\n",
        "\n",
        "incorrect_indices = np.nonzero(predicted_classes != y_test)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4coF9cdC4FuT"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "for i, correct in enumerate(correct_indices[:6]):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_test[correct]))\n",
        "\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAm5wIBo7C1n"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "for i, incorrect in enumerate(incorrect_indices[:6]):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_test[incorrect]))\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzB25C3M7NBh"
      },
      "source": [
        "# **Let's go Deeper**\n",
        "We will add four more layers to our model. We use Droupout in our model to reduce overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6z6Pj5z7mIv"
      },
      "outputs": [],
      "source": [
        "# Dropout helps protect the model from memorizing or \"overfitting\" the training data.\n",
        "Pkeep=0.25\n",
        "modelDeepFC = Sequential([\n",
        "    Dense(200, input_shape=(784,)),\n",
        "    Activation('relu'),\n",
        "    Dropout(Pkeep),\n",
        "    Dense(100, input_shape=(200,)),\n",
        "    Activation('relu'),\n",
        "    Dropout(Pkeep),\n",
        "    Dense(60, input_shape=(100,)),\n",
        "    Activation('relu'),\n",
        "    Dropout(Pkeep),\n",
        "    Dense(30, input_shape=(60,)),\n",
        "    Activation('relu'),\n",
        "    Dropout(Pkeep),\n",
        "    Dense(10),\n",
        "    Activation('softmax'),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSaOEmsg-pJE"
      },
      "outputs": [],
      "source": [
        "plotlossesdeeper = PlotLossesKeras()\n",
        "modelDeepFC.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "modelDeepFC.fit(X_train, Y_train,\n",
        "          validation_data=(X_val, Y_val),\n",
        "          epochs=20, batch_size=32,\n",
        "          verbose=1,\n",
        "          callbacks=[plotlossesdeeper])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh6KJZp56jDp"
      },
      "outputs": [],
      "source": [
        "score_deep = modelDeepFC.evaluate(X_test, Y_test)\n",
        "print('Test score:', score_deep[0])\n",
        "print('Test accuracy:', score_deep[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Gy_4HN0BWgq"
      },
      "source": [
        "The performance of the last model with more layers showed a better performance compare with our first model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_gM34gjBsAW"
      },
      "source": [
        "# **Let's go furter with introducing CNN**\n",
        "Before, we built a network that accepts the normalized pixel values of each value and operates soley on those values. What if we could instead feed different features (e.g. curvature, edges) of each image into a network, and have the network learn which features are important for classifying an image?\n",
        "\n",
        "This possible through convolution! Convolution applies kernels (filters) that traverse through each image and generate feature maps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HfNFU_ZCV7t"
      },
      "outputs": [],
      "source": [
        "# import some additional tools\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnyHx6KdCpDM"
      },
      "outputs": [],
      "source": [
        "# Reload the MNIST data\n",
        "(X_train, y_train), (X_val_test, y_val_test) = mnist.load_data()\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42) # 0.5 means half of the validation data will be used for testing. Adjust as needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3_HL1Z5CsMF"
      },
      "outputs": [],
      "source": [
        "# Again, do some formatting\n",
        "# Except we do not flatten each image into a 784-length vector because we want to perform convolutions first\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') #add an additional dimension to represent the single-channel\n",
        "X_val = X_val.reshape(X_val.shape[0], 28, 28, 1).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
        "\n",
        "X_train /= 255                              # normalize each value for each pixel for the entire vector for each input\n",
        "X_val /= 255\n",
        "X_test /= 255\n",
        "\n",
        "print(\"Training matrix shape\", X_train.shape)\n",
        "print(\"Validation matrix shape\", X_val.shape)\n",
        "print(\"Testing matrix shape\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VuttSwGDQdB"
      },
      "outputs": [],
      "source": [
        "# one hot encode outputs\n",
        "Y_train = to_categorical(y_train)\n",
        "Y_val = to_categorical(y_val)\n",
        "Y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsVz160NDdwF"
      },
      "outputs": [],
      "source": [
        "modelCNN = Sequential([\n",
        "\n",
        "    # Convolution Layer 1\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)), # 32 different 3x3 kernels -- so 32 feature maps\n",
        "    MaxPooling2D(pool_size=(2, 2)), # Pool the max values over a 2x2 kernel\n",
        "\n",
        "    # Convolution Layer 2\n",
        "    Conv2D(64, (3, 3), activation='relu'), # 64 different 3x3 kernels\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Convolution Layer 3\n",
        "    Conv2D(128, (3, 3), activation='relu'), # 128 different 3x3 kernels\n",
        "\n",
        "    Flatten(), # Flatten final 7x7x128 output matrix into a 1024-length vector\n",
        "\n",
        "    # Fully Connected Layer 4\n",
        "    Dense(512), # 512 FCN nodes\n",
        "    Activation('relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(10),\n",
        "    Activation('softmax'),\n",
        "])\n",
        "modelCNN.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2kzBiLRE3Hw"
      },
      "outputs": [],
      "source": [
        "plotlossesCNN = PlotLossesKeras()\n",
        "modelCNN.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "modelCNN.fit(X_train, Y_train,\n",
        "          validation_data=(X_val, Y_val),\n",
        "          epochs=20, batch_size=32,\n",
        "          verbose=1,\n",
        "          callbacks=[plotlossesCNN])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipCAB6BZMZjh"
      },
      "source": [
        "**Evaluation and Prediction**\n",
        "\n",
        "We can use our model to make a prediction on new images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNFXqmiSOMtp"
      },
      "outputs": [],
      "source": [
        "modelCNN.evaluate(X_test,Y_test) #Evaluation of the model on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5Lv4PC_FpaH"
      },
      "outputs": [],
      "source": [
        "predicted_classes = np.argmax(modelCNN.predict(X_test), axis=-1)  # Get predicted classes using argmax\n",
        "print(predicted_classes)  # Print the predicted classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-WoRJj5PMlt"
      },
      "outputs": [],
      "source": [
        "# Prediction of classes of a single image\n",
        "img=X_test[1000,:,:,:]\n",
        "img = np.array(img).reshape(-1, 28, 28, 1)\n",
        "output = np.argmax(modelCNN.predict(img), axis=-1)\n",
        "print('The predicted label is: ',output[0])\n",
        "print('The real label is: ',y_test[1000])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which items we got right / wrong\n",
        "correct_indices = np.nonzero(predicted_classes == y_test)[0]\n",
        "\n",
        "incorrect_indices = np.nonzero(predicted_classes != y_test)[0]"
      ],
      "metadata": {
        "id": "ikfppli5ApV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "for i, correct in enumerate(correct_indices[:3]):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(X_test[correct], cmap='gray', interpolation='none')\n",
        "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_test[correct]))\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "zokfGwUwA_Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "for i, incorrect in enumerate(incorrect_indices[:9]):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(X_test[incorrect], cmap='gray', interpolation='none')\n",
        "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_test[incorrect]))\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "zWW4HzX5BCcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3A96iC5PBP-Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}